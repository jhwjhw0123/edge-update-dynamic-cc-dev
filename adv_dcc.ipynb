{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "159e44ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "from utils import *\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import time\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "material-exposure",
   "metadata": {},
   "source": [
    "## generate SBMs and Erdos-Renyi graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "unlikely-action",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SBMGraphStream():\n",
    "    '''\n",
    "    The class of Graph Stream from the stochastic block model\n",
    "    ----- Parameters -----\n",
    "    # n_vertex: the number of vertices in the graph\n",
    "    # p_intra: the probability for + edge (u,v) for the same cluster\n",
    "    # p_inter: the probability for + edge (u,v) for different clusters\n",
    "    # k_cluster: number of clusters in the clustering\n",
    "    ----- Methods ----\n",
    "    # read_next_edge(): read the next edge and move the index +1\n",
    "    ----- Representation ----\n",
    "    The graph is representation with an indexed array of vertices and a dictionary with (u_i, u_j): labels\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_vertex, p_intra=0.8, p_inter=0.2, k_cluster=7):\n",
    "        '''\n",
    "        :param n_vertex: the the number of vertices in the graph\n",
    "        '''\n",
    "        self.n_vertex = n_vertex\n",
    "        self.p_intra = p_intra\n",
    "        self.p_inter = p_inter\n",
    "        self.k_cluster = k_cluster\n",
    "        \n",
    "        # initialize the vertex set and the cluster labels\n",
    "        self.vertex_set = np.array([self.n_vertex])\n",
    "        num_v_per_cluster = n_vertex//self.k_cluster\n",
    "        n_residual = n_vertex % num_v_per_cluster\n",
    "        cluster_labels_list = []\n",
    "        for i_cluster in range(k_cluster):\n",
    "            cluster_labels_list.append(i_cluster*np.ones([num_v_per_cluster]))\n",
    "        if n_residual!=0:\n",
    "            cluster_labels_list.append((k_cluster-1)*np.ones([n_residual]))\n",
    "        # collect them as a 1-d array\n",
    "        self.cluster_labels = np.reshape(np.hstack(cluster_labels_list).astype(int), [-1])\n",
    "        # initialize the edges -- using +1 and -1 to represent the edge labels\n",
    "        # also compute the cost\n",
    "        self.cc_cost = 0\n",
    "        self.edge_dict = {}\n",
    "        for u_i in tqdm.tqdm(range(self.n_vertex)):\n",
    "            for u_j in np.arange(u_i+1, self.n_vertex):\n",
    "                if self.cluster_labels[u_i] == self.cluster_labels[u_j]:\n",
    "                    if np.random.rand() <= p_intra:\n",
    "                        self.edge_dict[(u_i,u_j)] = 1\n",
    "                    else:\n",
    "                        self.edge_dict[(u_i,u_j)] = -1\n",
    "                        self.cc_cost = self.cc_cost + 1\n",
    "                else:\n",
    "                    if np.random.rand() <= p_inter:\n",
    "                        self.edge_dict[(u_i,u_j)] = 1\n",
    "                        self.cc_cost = self.cc_cost + 1\n",
    "                    else:\n",
    "                        self.edge_dict[(u_i,u_j)] = -1\n",
    "        # randomize the order of edge arrival\n",
    "        self.edge_names = list(self.edge_dict.keys())\n",
    "        random.shuffle(self.edge_names)\n",
    "        self.num_edges = len(self.edge_names)\n",
    "        # maintain a pointer of the number of edges\n",
    "        self.current_stream_ind = 0\n",
    "        \n",
    "    def read_next_edge(self):\n",
    "        \n",
    "        this_edge_name = self.edge_names[self.current_stream_ind]\n",
    "        this_edge_label = self.edge_dict[this_edge_name]\n",
    "        self.current_stream_ind = self.current_stream_ind + 1\n",
    "        if self.current_stream_ind>=self.num_edges-1:\n",
    "            return None, None\n",
    "        \n",
    "        return this_edge_name, this_edge_label\n",
    "    \n",
    "    def write_edges(self, write_path=None):\n",
    "        if not write_path:\n",
    "            raise ValueError('the writing path has to be specified!')\n",
    "        file_name = 'SBM_n='+str(self.n_vertex)+'_p='+str(self.p_intra)+'_k=' + str(self.k_cluster) +'.csv'\n",
    "        with open(os.path.join(write_path, file_name), 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            for edge in self.edge_dict:\n",
    "                if self.edge_dict[edge]>0:\n",
    "                    writer.writerow([f'{edge[0]} {edge[1]}'])\n",
    "    \n",
    "    def reset_index(self):\n",
    "        '''\n",
    "        reset the pointer\n",
    "        '''\n",
    "        self.current_stream_ind = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "outside-restoration",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1838.39it/s]\n"
     ]
    }
   ],
   "source": [
    "sbm_graph_stream = SBMGraphStream(n_vertex=1000,p_intra=0.95, p_inter=0.05, k_cluster=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "successful-future",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbm_graph_stream.write_edges(write_path='../data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-parts",
   "metadata": {},
   "source": [
    "## The functions that implements our algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "modern-brown",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sparse_vertex(current_graph, eps=0.2):\n",
    "    # sample log n neighbors for each vertex\n",
    "    current_num_vertex = len(current_graph)\n",
    "    num_sample = max((int)(5*np.log(current_num_vertex)/eps), 20)\n",
    "    sample_dict = {}\n",
    "    for this_vertex in current_graph:\n",
    "        sample_vertex_set = list(current_graph[this_vertex].getRandom(i=num_sample))\n",
    "        sample_dict[this_vertex] = sample_vertex_set\n",
    "    sparse_vertex_list = []\n",
    "    # test sparsity for each vertex\n",
    "    for this_vertex in current_graph:\n",
    "        num_neighbor_diff = 0\n",
    "        neighbors_this = sample_dict[this_vertex]\n",
    "        for comp_vertex in sample_dict[this_vertex]:\n",
    "            neighbors_comp = sample_dict[comp_vertex]\n",
    "            sample_vertex = np.random.choice(current_num_vertex, num_sample)\n",
    "            # the intersections of the neighbor\n",
    "            total_diff = 0\n",
    "            for test_vertex in sample_vertex:\n",
    "                if (test_vertex in current_graph[this_vertex]) and (test_vertex not in current_graph[comp_vertex]):\n",
    "                    total_diff = total_diff + 1\n",
    "                if (test_vertex in current_graph[comp_vertex]) and (test_vertex not in current_graph[this_vertex]):\n",
    "                    total_diff = total_diff + 1\n",
    "            if (total_diff>=eps*num_sample):\n",
    "                num_neighbor_diff = num_neighbor_diff + 1\n",
    "        if num_neighbor_diff >=eps*num_sample:\n",
    "#             print('==========================')\n",
    "#             print(num_neighbor_diff)\n",
    "#             print(num_sample)\n",
    "            sparse_vertex_list.append(this_vertex)\n",
    "    \n",
    "    # return the list of sparse vertices\n",
    "    return sparse_vertex_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "ceramic-pixel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_dense_decop(current_graph, eps=0.2):\n",
    "    current_num_vertex = len(current_graph)\n",
    "    # the returned clusters\n",
    "    SDD_clustering = {}\n",
    "    # check sparse vertices\n",
    "    current_sparse_vertice = test_sparse_vertex(current_graph, eps=eps)\n",
    "    for sparse_vertex in current_sparse_vertice:\n",
    "        SDD_clustering[sparse_vertex]=sparse_vertex\n",
    "    # sample from the dense vertices\n",
    "    dense_subgraph = {vertex: current_graph[vertex] \n",
    "                      for vertex in current_graph if vertex not in current_sparse_vertice}\n",
    "    anchor_vertex_dict = {}\n",
    "    for this_vertex in dense_subgraph:\n",
    "        # rejection sampling\n",
    "        dense_sample_prob = max((2*np.log(current_num_vertex))/(eps*dense_subgraph[this_vertex].degree), 0.02)\n",
    "        if dense_sample_prob>=np.random.uniform(low=0.0, high=1.0):\n",
    "            anchor_vertex_dict[this_vertex] = dense_subgraph[this_vertex]\n",
    "    # recursively form almost-cliques\n",
    "    num_sample = max((int)(5*np.log(current_num_vertex)), 20)\n",
    "    AC_dict = {}\n",
    "    # maintain a list of covered vertices\n",
    "    covered_AC_vertex = []\n",
    "    for this_anchor_vertex in anchor_vertex_dict:\n",
    "        if this_anchor_vertex in covered_AC_vertex:\n",
    "            continue\n",
    "        AC_dict[this_anchor_vertex] = []\n",
    "        SDD_clustering[this_anchor_vertex] = this_anchor_vertex  # assign to the cluster represented by self\n",
    "        covered_AC_vertex.append(this_anchor_vertex)\n",
    "        anchor_neighbor_samples = list(anchor_vertex_dict[this_anchor_vertex].getRandom(i=num_sample))\n",
    "        for candidate_vertex in anchor_vertex_dict[this_anchor_vertex]:\n",
    "            if (candidate_vertex in current_sparse_vertice) or (candidate_vertex in covered_AC_vertex):\n",
    "                continue\n",
    "            # test whether their symmetric difference is large enough\n",
    "            # the intersections of the neighbor\n",
    "            total_diff = 0\n",
    "            for anchor_neighbor in anchor_neighbor_samples:\n",
    "                if anchor_neighbor not in list(dense_subgraph[candidate_vertex]):\n",
    "                    total_diff = total_diff + 1\n",
    "#             anchor_neighbor_in_sample = np.intersect1d(sample_vertex, list(dense_subgraph[this_anchor_vertex]))\n",
    "#             cand_neighbor_in_sample = np.intersect1d(sample_vertex, list(dense_subgraph[candidate_vertex]))\n",
    "#             dif1 = np.setdiff1d(anchor_neighbor_in_sample, cand_neighbor_in_sample)\n",
    "#             dif2 = np.setdiff1d(cand_neighbor_in_sample, anchor_neighbor_in_sample)\n",
    "#             if sbm_graph_stream.cluster_labels[this_anchor_vertex]!=sbm_graph_stream.cluster_labels[candidate_vertex]:\n",
    "#                 print('============= Different cluster happens! ==================')\n",
    "#                 print(anchor_neighbor_samples)\n",
    "#                 print(cand_neighbor_samples)\n",
    "#                 print(len(dif1[0]))\n",
    "#                 print(len(dif2[0]))\n",
    "#             total_diff = len(dif1)+len(dif2)\n",
    "            if (total_diff<=2*eps*num_sample):\n",
    "                AC_dict[this_anchor_vertex].append(candidate_vertex)\n",
    "                SDD_clustering[candidate_vertex] = this_anchor_vertex # assign the candidate vertex to the anchor\n",
    "                covered_AC_vertex.append(candidate_vertex)\n",
    "            # this line is for debugging purpose -- remove later\n",
    "            else:\n",
    "                pass\n",
    "    # add codes to add vertices to the almost-cliques\n",
    "   \n",
    "    dense_vertex_list = [v for v in current_graph if v not in current_sparse_vertice]\n",
    "    \n",
    "    return current_sparse_vertice, AC_dict, SDD_clustering, anchor_vertex_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "direct-julian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def singleton_cluster_alg(current_graph):\n",
    "    return {vertex: vertex for vertex in current_graph}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-budapest",
   "metadata": {},
   "source": [
    "## Read the edges and maintain clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "97b030cf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "adjacency_list, edge_list = create_graph_from_csv(\"../data/SBM_n=1000_p=0.95_k=4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "2252f748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTODO\\n\\nSee if functions are doable\\n\\n'"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "TODO\n",
    "\n",
    "See if functions are doable\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "18b1a0e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "[175 253 316 425 966]\n",
      "0\n",
      "251\n",
      "******************************\n",
      "The number of almost-cliques is  9\n",
      "---------------------\n",
      "Houston we have a problem with (1, 175) and cluster ids : (0,None)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-236-76ba31fe2e37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The number of almost-cliques is '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malmost_cliques\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'---------------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mSDD_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrelation_clustering_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madjacency_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSDD_clustering\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0mpivot_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrelation_clustering_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madjacency_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpivot_clustering\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0msingleton_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrelation_clustering_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madjacency_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msingleton_clustering\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/research_postdoc_Rice_TAMU/Adversarial-robust-correlation-clustering/codes/edge-update-dynamic-cc-dev/utils.py\u001b[0m in \u001b[0;36mcorrelation_clustering_value\u001b[0;34m(graph, clustering)\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcluster_id_node\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcluster_id_neighbor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Houston we have a problem with ({node}, {neighbor}) and cluster ids : ({cluster_id_node},{cluster_id_neighbor})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;31m# Check if nodes belong to different clusters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "no_edges = len(edge_list)  # No. of edges\n",
    "\n",
    "prob_del = 0.2      # Probability to delete edge\n",
    "eps_param = 0.1\n",
    "\n",
    "current_graph = {}\n",
    "current_edge_list = []\n",
    "\n",
    "available_edge_list = np.random.permutation(edge_list).tolist()\n",
    "\n",
    "stream_length = (int)(0.5*no_edges)\n",
    "\n",
    "track_update_num = {}\n",
    "track_update_benckmark = {}\n",
    "\n",
    "for i in range(stream_length):\n",
    "    # Insertion\n",
    "    if available_edge_list: #\n",
    "        current_edge_list.append(available_edge_list[i])\n",
    "        u = available_edge_list[i][0]\n",
    "        v = available_edge_list[i][1]\n",
    "        if u not in current_graph.keys():\n",
    "            current_graph[u] = OptList()\n",
    "        current_graph[u].insert(v)\n",
    "        if v not in current_graph.keys():\n",
    "            current_graph[v] = OptList()\n",
    "        current_graph[v].insert(u)\n",
    "        available_edge_list.pop(0)\n",
    "        \n",
    "        # keep track of the benchmark for the updates\n",
    "        if u not in track_update_benckmark:\n",
    "            track_update_benckmark[u] = current_graph[u].degree\n",
    "        if v not in track_update_benckmark:\n",
    "            track_update_benckmark[v] = current_graph[v].degree\n",
    "        # update the tracking of the updates on u and v\n",
    "        if u not in track_update_num:\n",
    "            track_update_num[u] = 1\n",
    "        else:\n",
    "            track_update_num[u] = track_update_num[u] + 1\n",
    "            \n",
    "        if v not in track_update_num:\n",
    "            track_update_num[v] = 1\n",
    "        else:\n",
    "            track_update_num[v] = track_update_num[v] + 1\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        Code for SDD and PIVOT goes here\n",
    "        '''\n",
    "        if (track_update_num[u]>max(100, eps_param*track_update_benckmark[u])):\n",
    "            # tests\n",
    "            start_SDD = time.time()\n",
    "            current_sparse_vertex_list, almost_cliques, SDD_clustering, anchor_vertex_dict = sparse_dense_decop(adjacency_list, eps=0.2)\n",
    "            end_SDD = time.time()\n",
    "            start_pivot = time.time()\n",
    "            pivot_clustering = classical_pivot(adjacency_list)\n",
    "            end_pivot = time.time()\n",
    "            singleton_clustering = singleton_cluster_alg(adjacency_list)\n",
    "            # clear the number of updates\n",
    "            track_update_num[u] = 0\n",
    "            track_update_benckmark = current_graph[u].degree\n",
    "            # =========== TODO: add this as a test for whether the SDD succeeds ======== \n",
    "            all_vertex_list = [v for v in current_graph]\n",
    "            AC_vertex_list = []\n",
    "            for anchor_ver in almost_cliques.keys():\n",
    "                AC_vertex_list.append(anchor_ver)\n",
    "                for ac_ver in almost_cliques[anchor_ver]:\n",
    "                    AC_vertex_list.append(ac_ver)\n",
    "            AC_vertex_list = list(set(AC_vertex_list))\n",
    "            recovered_vertex = np.concatenate((AC_vertex_list, current_sparse_vertex_list))\n",
    "            print('===============================')\n",
    "            print(np.setdiff1d(all_vertex_list,recovered_vertex))\n",
    "            print(len(current_sparse_vertex_list))\n",
    "            print(len(anchor_vertex_dict.keys()))\n",
    "            print('******************************')\n",
    "            print('The number of almost-cliques is ', len(almost_cliques))\n",
    "            print('---------------------')\n",
    "            SDD_cost = correlation_clustering_value(adjacency_list, SDD_clustering)\n",
    "            pivot_cost = correlation_clustering_value(adjacency_list, pivot_clustering)\n",
    "            singleton_cost = correlation_clustering_value(adjacency_list, singleton_clustering)\n",
    "            print('SDD clustering cost is', SDD_cost, 'and the running time is', end_SDD-start_SDD)\n",
    "            print('Pivot clustering cost is', pivot_cost, 'and the running time is', end_pivot-start_pivot)\n",
    "            print('Singleton clustering cost is', singleton_cost)\n",
    "            print('The correct optimal clustering cost should be', sbm_graph_stream.cc_cost)\n",
    "            print('^^^^^^^^^^^^^^^^^^^^^^^')\n",
    "            for anchor in almost_cliques.keys():\n",
    "                print(almost_cliques[anchor])\n",
    "            break\n",
    "        \n",
    "#     else:\n",
    "#         # We have run out of edges to insert\n",
    "#         edge_to_delete = np.random.choice(current_edge_list)\n",
    "        \n",
    "#         u = edge_to_delete[0]\n",
    "#         v = edge_to_delete[1]\n",
    "#         current_graph[u].remove(v)\n",
    "#         current_graph[v].remove(u)\n",
    "        \n",
    "#         available_edge_list.extend(edge_to_delete)\n",
    "#         current_edge_list.remove(edge_to_delete)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "#     if np.random.binomial(1,prob_del):\n",
    "#         # Deletion\n",
    "#         print(current_edge_list)\n",
    "#         edge_to_delete = np.random.choice(current_edge_list)\n",
    "        \n",
    "#         u = edge_to_delete[0]\n",
    "#         v = edge_to_delete[1]\n",
    "#         current_graph[u].remove(v)\n",
    "#         current_graph[v].remove(u)\n",
    "        \n",
    "#         available_edge_list.extend(edge_to_delete)\n",
    "#         current_edge_list.remove(edge_to_delete)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-canadian",
   "metadata": {},
   "source": [
    "For of clustering is dict[vertex-name]: cluster-name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prerequisite-cosmetic",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
