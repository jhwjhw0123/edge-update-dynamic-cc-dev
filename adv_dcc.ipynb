{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "159e44ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "from utils import *\n",
    "from collections import defaultdict, Counter\n",
    "import os\n",
    "import time\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relevant-companion",
   "metadata": {},
   "source": [
    "## generate SBMs and Erdos-Renyi graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "intense-assist",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SBMGraphStream():\n",
    "    '''\n",
    "    The class of Graph Stream from the stochastic block model\n",
    "    ----- Parameters -----\n",
    "    # n_vertex: the number of vertices in the graph\n",
    "    # p_intra: the probability for + edge (u,v) for the same cluster\n",
    "    # p_inter: the probability for + edge (u,v) for different clusters\n",
    "    # k_cluster: number of clusters in the clustering\n",
    "    ----- Methods ----\n",
    "    # read_next_edge(): read the next edge and move the index +1\n",
    "    ----- Representation ----\n",
    "    The graph is representation with an indexed array of vertices and a dictionary with (u_i, u_j): labels\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_vertex, p_intra=0.8, p_inter=0.2, k_cluster=7):\n",
    "        '''\n",
    "        :param n_vertex: the the number of vertices in the graph\n",
    "        '''\n",
    "        self.n_vertex = n_vertex\n",
    "        self.p_intra = p_intra\n",
    "        self.p_inter = p_inter\n",
    "        self.k_cluster = k_cluster\n",
    "        \n",
    "        # initialize the vertex set and the cluster labels\n",
    "        self.vertex_set = np.array([self.n_vertex])\n",
    "        num_v_per_cluster = n_vertex//self.k_cluster\n",
    "        n_residual = n_vertex % num_v_per_cluster\n",
    "        cluster_labels_list = []\n",
    "        for i_cluster in range(k_cluster):\n",
    "            cluster_labels_list.append(i_cluster*np.ones([num_v_per_cluster]))\n",
    "        if n_residual!=0:\n",
    "            cluster_labels_list.append((k_cluster-1)*np.ones([n_residual]))\n",
    "        # collect them as a 1-d array\n",
    "        self.cluster_labels = np.reshape(np.hstack(cluster_labels_list).astype(int), [-1])\n",
    "        # initialize the edges -- using +1 and -1 to represent the edge labels\n",
    "        # also compute the cost\n",
    "        self.cc_cost = 0\n",
    "        self.edge_dict = {}\n",
    "        for u_i in tqdm.tqdm(range(self.n_vertex)):\n",
    "            for u_j in np.arange(u_i+1, self.n_vertex):\n",
    "                if self.cluster_labels[u_i] == self.cluster_labels[u_j]:\n",
    "                    if np.random.rand() <= p_intra:\n",
    "                        self.edge_dict[(u_i,u_j)] = 1\n",
    "                    else:\n",
    "                        self.edge_dict[(u_i,u_j)] = -1\n",
    "                        self.cc_cost = self.cc_cost + 1\n",
    "                else:\n",
    "                    if np.random.rand() <= p_inter:\n",
    "                        self.edge_dict[(u_i,u_j)] = 1\n",
    "                        self.cc_cost = self.cc_cost + 1\n",
    "                    else:\n",
    "                        self.edge_dict[(u_i,u_j)] = -1\n",
    "        # randomize the order of edge arrival\n",
    "        self.edge_names = list(self.edge_dict.keys())\n",
    "        random.shuffle(self.edge_names)\n",
    "        self.num_edges = len(self.edge_names)\n",
    "        # maintain a pointer of the number of edges\n",
    "        self.current_stream_ind = 0\n",
    "        \n",
    "    def read_next_edge(self):\n",
    "        \n",
    "        this_edge_name = self.edge_names[self.current_stream_ind]\n",
    "        this_edge_label = self.edge_dict[this_edge_name]\n",
    "        self.current_stream_ind = self.current_stream_ind + 1\n",
    "        if self.current_stream_ind>=self.num_edges-1:\n",
    "            return None, None\n",
    "        \n",
    "        return this_edge_name, this_edge_label\n",
    "    \n",
    "    def write_edges(self, write_path=None):\n",
    "        if not write_path:\n",
    "            raise ValueError('the writing path has to be specified!')\n",
    "        file_name = 'SBM_n='+str(self.n_vertex)+'_p='+str(self.p_intra)+'_k=' + str(self.k_cluster) +'.csv'\n",
    "        with open(os.path.join(write_path, file_name), 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            for edge in self.edge_dict:\n",
    "                if self.edge_dict[edge]>0:\n",
    "                    writer.writerow([f'{edge[0]} {edge[1]}'])\n",
    "    \n",
    "    def reset_index(self):\n",
    "        '''\n",
    "        reset the pointer\n",
    "        '''\n",
    "        self.current_stream_ind = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "reduced-enforcement",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1838.39it/s]\n"
     ]
    }
   ],
   "source": [
    "sbm_graph_stream = SBMGraphStream(n_vertex=1000,p_intra=0.95, p_inter=0.05, k_cluster=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "stylish-engineering",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbm_graph_stream.write_edges(write_path='../data/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-parts",
   "metadata": {},
   "source": [
    "## The functions that implements our algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "modern-brown",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sparse_vertex(current_graph, eps=0.2):\n",
    "    # sample log n neighbors for each vertex\n",
    "    current_num_vertex = len(current_graph)\n",
    "    num_sample = max((int)(np.log(current_num_vertex)/eps), 20)\n",
    "    num_diff_test_sample = max((int)(np.log(current_num_vertex)/eps), 20)\n",
    "    sample_dict = {}\n",
    "    for this_vertex in current_graph:\n",
    "        sample_vertex_set = list(current_graph[this_vertex].getRandom(i=num_sample))\n",
    "        sample_dict[this_vertex] = sample_vertex_set\n",
    "    sparse_vertex_list = []\n",
    "    # test sparsity for each vertex\n",
    "    for this_vertex in current_graph:\n",
    "        num_neighbor_diff = 0\n",
    "        neighbors_this = sample_dict[this_vertex]\n",
    "        for comp_vertex in sample_dict[this_vertex]:\n",
    "            neighbors_comp = sample_dict[comp_vertex]\n",
    "            sample_vertex = np.random.choice(current_num_vertex, num_diff_test_sample)\n",
    "            # the intersections of the neighbor\n",
    "            total_diff = 0\n",
    "            for test_vertex in sample_vertex:\n",
    "                if (test_vertex in current_graph[this_vertex]) and (test_vertex not in current_graph[comp_vertex]):\n",
    "                    total_diff = total_diff + 1\n",
    "                if (test_vertex in current_graph[comp_vertex]) and (test_vertex not in current_graph[this_vertex]):\n",
    "                    total_diff = total_diff + 1\n",
    "            if (total_diff>=eps*num_diff_test_sample):\n",
    "                num_neighbor_diff = num_neighbor_diff + 1\n",
    "        if num_neighbor_diff >=eps*num_sample:\n",
    "#             print('==========================')\n",
    "#             print(num_neighbor_diff)\n",
    "#             print(num_sample)\n",
    "            sparse_vertex_list.append(this_vertex)\n",
    "    \n",
    "    # return the list of sparse vertices\n",
    "    return sparse_vertex_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "ceramic-pixel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_dense_decop(current_graph, eps=0.2):\n",
    "    current_num_vertex = len(current_graph)\n",
    "    # the returned clusters\n",
    "    SDD_clustering = {}\n",
    "    # check sparse vertices\n",
    "    tic = time.time()\n",
    "    current_sparse_vertice = test_sparse_vertex(current_graph, eps=eps)\n",
    "    for sparse_vertex in current_sparse_vertice:\n",
    "        SDD_clustering[sparse_vertex]=sparse_vertex\n",
    "    print('The time for sparsity testing is ', time.time()-tic)\n",
    "    # sample from the dense vertices\n",
    "    dense_subgraph = {vertex: current_graph[vertex] \n",
    "                      for vertex in current_graph if vertex not in current_sparse_vertice}\n",
    "    anchor_vertex_dict = {}\n",
    "    # tic = time.time()\n",
    "    for this_vertex in dense_subgraph:\n",
    "        # rejection sampling\n",
    "        dense_sample_prob = max((np.log(current_num_vertex))/(eps*dense_subgraph[this_vertex].degree), 0.02)\n",
    "        if dense_sample_prob>=np.random.uniform(low=0.0, high=1.0):\n",
    "            anchor_vertex_dict[this_vertex] = dense_subgraph[this_vertex]\n",
    "    # print('The time for indexing the subgraph ', time.time()-tic)\n",
    "    # recursively form almost-cliques\n",
    "    num_sample = max((int)(5*np.log(current_num_vertex)), 20)\n",
    "    AC_dict = {}\n",
    "    # maintain a list of covered vertices\n",
    "    # tic = time.time()\n",
    "    covered_AC_vertex = []\n",
    "    for this_anchor_vertex in anchor_vertex_dict:\n",
    "        if this_anchor_vertex in covered_AC_vertex:\n",
    "            continue\n",
    "        AC_dict[this_anchor_vertex] = []\n",
    "        SDD_clustering[this_anchor_vertex] = this_anchor_vertex  # assign to the cluster represented by self\n",
    "        covered_AC_vertex.append(this_anchor_vertex)\n",
    "        anchor_neighbor_samples = list(anchor_vertex_dict[this_anchor_vertex].getRandom(i=num_sample))\n",
    "        for candidate_vertex in anchor_vertex_dict[this_anchor_vertex]:\n",
    "            if (candidate_vertex in current_sparse_vertice) or (candidate_vertex in covered_AC_vertex):\n",
    "                continue\n",
    "            # test whether their symmetric difference is large enough\n",
    "            # the intersections of the neighbor\n",
    "            total_diff = 0\n",
    "            for anchor_neighbor in anchor_neighbor_samples:\n",
    "                if anchor_neighbor not in list(dense_subgraph[candidate_vertex]):\n",
    "                    total_diff = total_diff + 1\n",
    "#             anchor_neighbor_in_sample = np.intersect1d(sample_vertex, list(dense_subgraph[this_anchor_vertex]))\n",
    "#             cand_neighbor_in_sample = np.intersect1d(sample_vertex, list(dense_subgraph[candidate_vertex]))\n",
    "#             dif1 = np.setdiff1d(anchor_neighbor_in_sample, cand_neighbor_in_sample)\n",
    "#             dif2 = np.setdiff1d(cand_neighbor_in_sample, anchor_neighbor_in_sample)\n",
    "#             if sbm_graph_stream.cluster_labels[this_anchor_vertex]!=sbm_graph_stream.cluster_labels[candidate_vertex]:\n",
    "#                 print('============= Different cluster happens! ==================')\n",
    "#                 print(anchor_neighbor_samples)\n",
    "#                 print(cand_neighbor_samples)\n",
    "#                 print(len(dif1[0]))\n",
    "#                 print(len(dif2[0]))\n",
    "#             total_diff = len(dif1)+len(dif2)\n",
    "            if (total_diff<=1.5*eps*num_sample):\n",
    "                AC_dict[this_anchor_vertex].append(candidate_vertex)\n",
    "                SDD_clustering[candidate_vertex] = this_anchor_vertex # assign the candidate vertex to the anchor\n",
    "                covered_AC_vertex.append(candidate_vertex)\n",
    "            # this line is for debugging purpose -- remove later\n",
    "            else:\n",
    "                pass\n",
    "    # add codes to add vertices to the almost-cliques\n",
    "    # print('The time for forming almost-cliques is ', time.time()-tic)\n",
    "    # merge undecided vertices to the almost-cliques\n",
    "    num_diff_test_sample = max((int)(2*np.log(current_num_vertex)), 20)\n",
    "    undecided_AC_vertices = [dense_vertex for dense_vertex in dense_subgraph.keys() if dense_vertex not in covered_AC_vertex]\n",
    "    for und_vertex in undecided_AC_vertices:\n",
    "        und_v_sampled_neighbors = list(dense_subgraph[und_vertex].getRandom(i=num_diff_test_sample))\n",
    "        neighbor_AC_names = []\n",
    "        for merge_test_vertex in und_v_sampled_neighbors:\n",
    "            if merge_test_vertex not in SDD_clustering:\n",
    "                continue\n",
    "            neighbor_AC_names.append(SDD_clustering[merge_test_vertex])\n",
    "        AC_counts = Counter(neighbor_AC_names)\n",
    "        most_common_AC, most_common_AC_freq = AC_counts.most_common(1)[0]\n",
    "        if most_common_AC_freq>=(1-2*eps)*num_diff_test_sample:\n",
    "            SDD_clustering[und_vertex] = most_common_AC\n",
    "            AC_dict[most_common_AC].append(und_vertex)\n",
    "        else:\n",
    "            # this line should not happen, but just add in case of strange bugs\n",
    "            current_sparse_vertice.append(und_vertex)\n",
    "    \n",
    "    dense_vertex_list = [v for v in current_graph if v not in current_sparse_vertice]\n",
    "    \n",
    "    return current_sparse_vertice, AC_dict, SDD_clustering, anchor_vertex_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "marine-browse",
   "metadata": {},
   "outputs": [],
   "source": [
    "def singleton_cluster_alg(current_graph):\n",
    "    return {vertex: vertex for vertex in current_graph}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-budapest",
   "metadata": {},
   "source": [
    "## Read the edges and maintain clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "97b030cf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "adjacency_list, edge_list = create_graph_from_csv(\"../data/SBM_n=1000_p=0.95_k=4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "2252f748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTODO\\n\\nSee if functions are doable\\n\\n'"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "TODO\n",
    "\n",
    "See if functions are doable\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "18b1a0e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time for sparsity testing is  0.6012520790100098\n",
      "list of undecided AC vertice [21, 25, 92, 105, 114, 144, 177, 482, 489, 721, 257, 309, 452, 550, 925, 496, 885, 891, 882, 538, 548, 528, 560, 627, 443, 845, 589, 606, 564, 664, 671, 570, 769, 355, 334, 373]\n",
      "===============================\n",
      "[ 21  25  92 105 114 144 177 257 309 334 355 373 443 452 482 489 496 528\n",
      " 538 548 550 560 564 570 589 606 627 664 671 721 769 845 882 885 891 925]\n",
      "0\n",
      "46\n",
      "******************************\n",
      "The number of almost-cliques is  4\n",
      "---------------------\n",
      "SDD clustering cost is 55008.0 and the running time is 3.220918893814087\n",
      "Pivot clustering cost is 56448.0 and the running time is 0.0017857551574707031\n",
      "Singleton clustering cost is 136803.0\n",
      "The correct optimal clustering cost should be 24819\n"
     ]
    }
   ],
   "source": [
    "no_edges = len(edge_list)  # No. of edges\n",
    "\n",
    "prob_del = 0.2      # Probability to delete edge\n",
    "eps_param = 0.1\n",
    "\n",
    "current_graph = {}\n",
    "current_edge_list = []\n",
    "\n",
    "available_edge_list = np.random.permutation(edge_list).tolist()\n",
    "\n",
    "stream_length = (int)(0.5*no_edges)\n",
    "\n",
    "track_update_num = {}\n",
    "track_update_benckmark = {}\n",
    "\n",
    "for i in range(stream_length):\n",
    "    # Insertion\n",
    "    if available_edge_list: #\n",
    "        current_edge_list.append(available_edge_list[i])\n",
    "        u = available_edge_list[i][0]\n",
    "        v = available_edge_list[i][1]\n",
    "        if u not in current_graph.keys():\n",
    "            current_graph[u] = OptList()\n",
    "        current_graph[u].insert(v)\n",
    "        if v not in current_graph.keys():\n",
    "            current_graph[v] = OptList()\n",
    "        current_graph[v].insert(u)\n",
    "        available_edge_list.pop(0)\n",
    "        \n",
    "        # keep track of the benchmark for the updates\n",
    "        if u not in track_update_benckmark:\n",
    "            track_update_benckmark[u] = current_graph[u].degree\n",
    "        if v not in track_update_benckmark:\n",
    "            track_update_benckmark[v] = current_graph[v].degree\n",
    "        # update the tracking of the updates on u and v\n",
    "        if u not in track_update_num:\n",
    "            track_update_num[u] = 1\n",
    "        else:\n",
    "            track_update_num[u] = track_update_num[u] + 1\n",
    "            \n",
    "        if v not in track_update_num:\n",
    "            track_update_num[v] = 1\n",
    "        else:\n",
    "            track_update_num[v] = track_update_num[v] + 1\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        Code for SDD and PIVOT goes here\n",
    "        '''\n",
    "        if (track_update_num[u]>max(100, eps_param*track_update_benckmark[u])):\n",
    "            # tests\n",
    "            start_SDD = time.time()\n",
    "            current_sparse_vertex_list, almost_cliques, SDD_clustering, anchor_vertex_dict = sparse_dense_decop(adjacency_list, eps=0.5)\n",
    "            end_SDD = time.time()\n",
    "            start_pivot = time.time()\n",
    "            pivot_clustering = classical_pivot(adjacency_list)\n",
    "            end_pivot = time.time()\n",
    "            singleton_clustering = singleton_cluster_alg(adjacency_list)\n",
    "            # clear the number of updates\n",
    "            track_update_num[u] = 0\n",
    "            track_update_benckmark = current_graph[u].degree\n",
    "            # =========== TODO: add this as a test for whether the SDD succeeds ======== \n",
    "            all_vertex_list = [v for v in current_graph]\n",
    "            AC_vertex_list = []\n",
    "            for anchor_ver in almost_cliques.keys():\n",
    "                AC_vertex_list.append(anchor_ver)\n",
    "                for ac_ver in almost_cliques[anchor_ver]:\n",
    "                    AC_vertex_list.append(ac_ver)\n",
    "            AC_vertex_list = list(set(AC_vertex_list))\n",
    "            recovered_vertex = np.concatenate((AC_vertex_list, current_sparse_vertex_list))\n",
    "            print('===============================')\n",
    "            print(np.setdiff1d(all_vertex_list,recovered_vertex))\n",
    "            print(len(current_sparse_vertex_list))\n",
    "            print(len(anchor_vertex_dict.keys()))\n",
    "            print('******************************')\n",
    "            print('The number of almost-cliques is ', len(almost_cliques))\n",
    "            print('---------------------')\n",
    "            SDD_cost = correlation_clustering_value(adjacency_list, SDD_clustering)\n",
    "            pivot_cost = correlation_clustering_value(adjacency_list, pivot_clustering)\n",
    "            singleton_cost = correlation_clustering_value(adjacency_list, singleton_clustering)\n",
    "            print('SDD clustering cost is', SDD_cost, 'and the running time is', end_SDD-start_SDD)\n",
    "            print('Pivot clustering cost is', pivot_cost, 'and the running time is', end_pivot-start_pivot)\n",
    "            print('Singleton clustering cost is', singleton_cost)\n",
    "            print('The correct optimal clustering cost should be', sbm_graph_stream.cc_cost)\n",
    "            break\n",
    "        \n",
    "#     else:\n",
    "#         # We have run out of edges to insert\n",
    "#         edge_to_delete = np.random.choice(current_edge_list)\n",
    "        \n",
    "#         u = edge_to_delete[0]\n",
    "#         v = edge_to_delete[1]\n",
    "#         current_graph[u].remove(v)\n",
    "#         current_graph[v].remove(u)\n",
    "        \n",
    "#         available_edge_list.extend(edge_to_delete)\n",
    "#         current_edge_list.remove(edge_to_delete)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "#     if np.random.binomial(1,prob_del):\n",
    "#         # Deletion\n",
    "#         print(current_edge_list)\n",
    "#         edge_to_delete = np.random.choice(current_edge_list)\n",
    "        \n",
    "#         u = edge_to_delete[0]\n",
    "#         v = edge_to_delete[1]\n",
    "#         current_graph[u].remove(v)\n",
    "#         current_graph[v].remove(u)\n",
    "        \n",
    "#         available_edge_list.extend(edge_to_delete)\n",
    "#         current_edge_list.remove(edge_to_delete)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-license",
   "metadata": {},
   "source": [
    "For of clustering is dict[vertex-name]: cluster-name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prerequisite-cosmetic",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
